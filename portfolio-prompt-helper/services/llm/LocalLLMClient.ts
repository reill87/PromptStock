/**
 * LocalLLMClient
 * ë¡œì»¬ ë””ë°”ì´ìŠ¤ì—ì„œ LLMì„ ì‹¤í–‰í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸
 * llama.rnì„ ì‚¬ìš©í•˜ì—¬ Vision LLM ëª¨ë¸ ì‹¤í–‰
 */

import { Platform } from 'react-native';
import * as FileSystem from 'expo-file-system/legacy';
import { initLlama } from 'llama.rn';
import type { LlamaContext } from 'llama.rn';
import { LLMClient, LLMResponse, LLMGenerationProgress } from '@/types/llm';

// íƒ€ì„ì•„ì›ƒ ìƒìˆ˜ (5ë¶„)
const GENERATION_TIMEOUT_MS = 5 * 60 * 1000;

/**
 * íƒ€ì„ì•„ì›ƒ ë˜í¼ í•¨ìˆ˜
 */
function withTimeout<T>(
  promise: Promise<T>,
  timeoutMs: number,
  errorMessage: string
): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) =>
      setTimeout(() => reject(new Error(errorMessage)), timeoutMs)
    ),
  ]);
}

export class LocalLLMClient implements LLMClient {
  name = 'Vision LLM (ë¡œì»¬)';
  mode = 'local' as const;
  supportsImages = true;

  private context: LlamaContext | null = null;
  private onProgress?: (progress: LLMGenerationProgress) => void;

  constructor(
    private modelPath: string,
    private mmprojPath: string,
    private config: {
      maxTokens?: number;
      temperature?: number;
      contextSize?: number;
    } = {},
    progressCallback?: (progress: LLMGenerationProgress) => void
  ) {
    this.onProgress = progressCallback;
  }

  /**
   * ëª¨ë¸ ì´ˆê¸°í™”
   * llama.rnì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ vision projector ë¡œë“œ
   */
  async initialize(): Promise<void> {
    if (Platform.OS === 'web') {
      throw new Error('ë¡œì»¬ LLMì€ ì›¹ í”Œë«í¼ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤');
    }

    if (this.context) {
      console.warn('Context already initialized');
      return;
    }

    try {
      this.onProgress?.({
        stage: 'initializing',
        progress: 5,
        message: 'ëª¨ë¸ íŒŒì¼ í™•ì¸ ì¤‘...',
      });

      // ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
      const modelExists = await FileSystem.getInfoAsync(this.modelPath);
      if (!modelExists.exists) {
        throw new Error(
          `ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: ${this.modelPath}\n\nëª¨ë¸ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.`
        );
      }

      const mmprojExists = await FileSystem.getInfoAsync(this.mmprojPath);
      if (!mmprojExists.exists) {
        throw new Error(
          `Vision ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: ${this.mmprojPath}\n\nëª¨ë¸ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.`
        );
      }

      this.onProgress?.({
        stage: 'initializing',
        progress: 20,
        message: 'ëª¨ë¸ ë¡œë”© ì¤‘...',
      });

      console.log('Initializing llama.rn with:', {
        model: this.modelPath,
        mmproj: this.mmprojPath,
        contextSize: this.config.contextSize || 2048,
      });

      // Step 1: llama.rn ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” (íƒ€ì„ì•„ì›ƒ 2ë¶„)
      this.context = await withTimeout(
        initLlama({
          model: this.modelPath,
          // mmprojëŠ” initMultimodalì—ì„œ ë³„ë„ë¡œ ë¡œë“œ
          use_mlock: true, // ë©”ëª¨ë¦¬ ì ê¸ˆ (ì„±ëŠ¥ í–¥ìƒ)
          n_ctx: this.config.contextSize || 2048, // ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
          n_gpu_layers: 0, // CPUë§Œ ì‚¬ìš© (ë°°í„°ë¦¬ ê³ ë ¤)
          ctx_shift: false, // Multimodal í•„ìˆ˜: ë¯¸ë””ì–´ í† í° ìœ„ì¹˜ ìœ ì§€
          seed: 42, // ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        }),
        120000,
        'ëª¨ë¸ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (2ë¶„). ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ëª¨ë¸ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
      );

      this.onProgress?.({
        stage: 'initializing',
        progress: 60,
        message: 'Vision ëª¨ë¸ ë¡œë”© ì¤‘...',
      });

      // Step 2: Multimodal (Vision) ì´ˆê¸°í™”
      console.log('ğŸ”§ Starting multimodal initialization...');
      console.log('ğŸ“‚ mmproj path:', this.mmprojPath);

      const multimodalSuccess = await withTimeout(
        this.context.initMultimodal({
          path: this.mmprojPath,
          use_gpu: true, // GPU ì‚¬ìš© (ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ)
        }),
        60000,
        'Vision ëª¨ë¸ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (1ë¶„). mmproj íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
      );

      console.log('âœ… initMultimodal returned:', multimodalSuccess);

      // Multimodal í™œì„±í™” í™•ì¸
      const isEnabled = await this.context.isMultimodalEnabled();
      console.log('ğŸ” isMultimodalEnabled:', isEnabled);

      if (!isEnabled) {
        throw new Error(
          'Multimodal ì´ˆê¸°í™” ì‹¤íŒ¨. mmproj íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ëª¨ë¸ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n' +
          'í•´ê²° ë°©ë²•:\n' +
          '1. ëª¨ë¸ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”\n' +
          '2. LLaVA 1.5 7B Q4 ëª¨ë¸ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”'
        );
      }

      // Multimodal ì§€ì› í™•ì¸
      const multimodalSupport = await this.context.getMultimodalSupport();
      console.log('ğŸ“Š Multimodal support:', multimodalSupport);

      this.onProgress?.({
        stage: 'initializing',
        progress: 100,
        message: 'ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ',
      });

      console.log('LocalLLMClient initialized successfully with vision support');
    } catch (error: any) {
      console.error('Failed to initialize llama.rn:', error);
      this.context = null;

      // ì—ëŸ¬ íƒ€ì…ë³„ë¡œ ë‹¤ë¥¸ ë©”ì‹œì§€ ì œê³µ
      if (error.message.includes('ë©”ëª¨ë¦¬')) {
        throw new Error(
          'ë©”ëª¨ë¦¬ ë¶€ì¡±: ë””ë°”ì´ìŠ¤ RAMì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
        );
      } else if (error.message.includes('ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')) {
        throw error; // ì´ë¯¸ ëª…í™•í•œ ë©”ì‹œì§€
      } else if (error.message.includes('ì‹œê°„ ì´ˆê³¼')) {
        throw error; // íƒ€ì„ì•„ì›ƒ ë©”ì‹œì§€
      } else {
        throw new Error(
          `ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: ${error.message}\n\nì•±ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ëª¨ë¸ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.`
        );
      }
    }
  }

  /**
   * í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ë° ì‘ë‹µ ìƒì„±
   *
   * @param prompt í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
   * @param images ì´ë¯¸ì§€ ë°°ì—´ (base64 ì¸ì½”ë”©)
   * @returns LLM ì‘ë‹µ
   */
  async generate(prompt: string, images?: string[]): Promise<LLMResponse> {
    // ì…ë ¥ ê²€ì¦
    if (!prompt || prompt.trim().length === 0) {
      throw new Error('í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.');
    }

    if (prompt.length > 10000) {
      throw new Error(
        'í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 10,000ì ì´í•˜ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.'
      );
    }

    if (!this.context) {
      await this.initialize();
    }

    const startTime = Date.now();

    try {
      // ì´ë¯¸ì§€ ì²˜ë¦¬ ë‹¨ê³„
      if (images && images.length > 0) {
        this.onProgress?.({
          stage: 'processing-images',
          progress: 30,
          message: `ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘... (${images.length}ê°œ)`,
        });

        // ì´ë¯¸ì§€ ê°œìˆ˜ ì œí•œ
        if (images.length > 10) {
          throw new Error(
            'ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì§€ì›í•©ë‹ˆë‹¤.'
          );
        }
      }

      // ì‘ë‹µ ìƒì„± ë‹¨ê³„
      this.onProgress?.({
        stage: 'generating',
        progress: 50,
        message: 'ì‘ë‹µ ìƒì„± ì¤‘...',
      });

      // ì´ë¯¸ì§€ë¥¼ data URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const imageDataURLs = images?.map((base64) => {
        // base64ê°€ ì´ë¯¸ data URL í˜•ì‹ì¸ì§€ í™•ì¸
        if (base64.startsWith('data:')) {
          return base64;
        }
        // ì•„ë‹ˆë©´ data URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        return `data:image/jpeg;base64,${base64}`;
      });

      // ğŸ” ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸
      console.log('========== LocalLLM Generation Debug ==========');
      console.log('ğŸ“ Original prompt length:', prompt.length);
      console.log('ğŸ“ Original prompt preview:', prompt.substring(0, 100) + '...');
      console.log('ğŸ–¼ï¸  Images received:', images?.length || 0);
      console.log('ğŸ–¼ï¸  Image data URLs created:', imageDataURLs?.length || 0);

      if (imageDataURLs && imageDataURLs.length > 0) {
        console.log('ğŸ–¼ï¸  First image info:', {
          startsWithData: imageDataURLs[0].startsWith('data:'),
          length: imageDataURLs[0].length,
          prefix: imageDataURLs[0].substring(0, 50) + '...'
        });
      }

      // messages í˜•ì‹ìœ¼ë¡œ content êµ¬ì„±
      // llama.rnì€ messages APIë¥¼ í†µí•´ ìë™ìœ¼ë¡œ LLaVA í…œí”Œë¦¿ ì ìš©
      const messageContent: any[] = [];

      // ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
      if (imageDataURLs && imageDataURLs.length > 0) {
        imageDataURLs.forEach((url) => {
          messageContent.push({
            type: 'image_url',
            image_url: { url },
          });
        });
      }

      // í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
      messageContent.push({
        type: 'text',
        text: prompt,
      });

      console.log('ğŸ“‹ Message content structure:', {
        imageCount: imageDataURLs?.length || 0,
        hasText: true,
        totalContentItems: messageContent.length,
      });
      console.log('âš™ï¸  Completion params:', {
        maxTokens: this.config.maxTokens || 2048,
        temperature: this.config.temperature || 0.7,
        usingMessagesAPI: true,
      });
      console.log('===============================================');

      // llama.rn completion ì‹¤í–‰ (messages API ì‚¬ìš©, íƒ€ì„ì•„ì›ƒ 5ë¶„)
      const result = await withTimeout(
        this.context!.completion({
          messages: [
            {
              role: 'user',
              content: messageContent,
            },
          ],
          n_predict: this.config.maxTokens || 2048, // 512 â†’ 2048ë¡œ ì¦ê°€
          temperature: this.config.temperature || 0.7,
          top_k: 40,
          top_p: 0.95,
          stop: ['</s>'], // '\n\n\n' ì œê±° - ë„ˆë¬´ ì¼ì° ì¤‘ë‹¨ë¨
        }),
        GENERATION_TIMEOUT_MS,
        'ì‘ë‹µ ìƒì„± ì‹œê°„ ì´ˆê³¼ (5ë¶„). í”„ë¡¬í”„íŠ¸ë¥¼ ë” ì§§ê²Œ í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”.'
      );

      this.onProgress?.({
        stage: 'completed',
        progress: 100,
        message: 'ì™„ë£Œ',
      });

      const processingTime = Date.now() - startTime;

      console.log('========== Generation Result Debug ==========');
      console.log(`â±ï¸  Processing time: ${processingTime}ms`);
      console.log('ğŸ“Š Result stats:', {
        tokenCount: result.tokens?.length,
        textLength: result.text.length,
      });
      console.log('ğŸ“„ Generated text (first 200 chars):');
      console.log(result.text.substring(0, 200));
      console.log('============================================');

      // ë¹ˆ ì‘ë‹µ ì²´í¬
      const responseText = result.text.trim();
      if (!responseText) {
        throw new Error(
          'ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
        );
      }

      return {
        text: responseText,
        processingTime,
        modelUsed: this.name,
        tokenCount: result.tokens?.length,
      };
    } catch (error: any) {
      console.error('Generation failed:', error);

      // ì—ëŸ¬ íƒ€ì…ë³„ë¡œ ë‹¤ë¥¸ ë©”ì‹œì§€ ì œê³µ
      if (error.message.includes('ì‹œê°„ ì´ˆê³¼')) {
        throw error; // íƒ€ì„ì•„ì›ƒ ë©”ì‹œì§€
      } else if (error.message.includes('ë©”ëª¨ë¦¬')) {
        throw new Error(
          'ë©”ëª¨ë¦¬ ë¶€ì¡±: ë””ë°”ì´ìŠ¤ RAMì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
        );
      } else if (error.message.includes('ë„ˆë¬´')) {
        throw error; // ì…ë ¥ í¬ê¸° ê´€ë ¨ ë©”ì‹œì§€
      } else {
        throw new Error(
          `ë¶„ì„ ì‹¤íŒ¨: ${error.message}\n\nì•±ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ì„¤ì •ì„ ì¡°ì •í•´ì£¼ì„¸ìš”.`
        );
      }
    }
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   * ë©”ëª¨ë¦¬ í•´ì œ ë° ëª¨ë¸ ì–¸ë¡œë“œ
   */
  async cleanup(): Promise<void> {
    if (this.context) {
      try {
        await this.context.release();
        console.log('LocalLLMClient context released');
      } catch (error) {
        console.error('Failed to release context:', error);
      } finally {
        this.context = null;
      }
    }
  }

  /**
   * ì¤€ë¹„ ìƒíƒœ í™•ì¸
   */
  isReady(): boolean {
    return this.context !== null;
  }

  /**
   * ìƒì„± ì¤‘ë‹¨
   * ê¸´ ìƒì„± ì‘ì—…ì„ ì¤‘ë‹¨í•  ë•Œ ì‚¬ìš©
   */
  async stopGeneration(): Promise<void> {
    if (this.context) {
      try {
        await this.context.stopCompletion();
        console.log('Generation stopped');
      } catch (error) {
        console.error('Failed to stop generation:', error);
      }
    }
  }
}
